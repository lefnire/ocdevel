export * from './meta.js'

See [Andrew Ng Week 3 Lecture Notes](https://www.coursera.org/learn/machine-learning/resources/Zi29t)

## Overview
- **Logistic Function**: A sigmoid function transforming linear regression output to logits, providing a probability between 0 and 1.
- **Binary Classification**: Logistic regression deals with binary outcomes, determining either 0 or 1 based on a threshold (e.g., 0.5).
- **Error Function**: Uses log likelihood to measure the accuracy of predictions in logistic regression.
- **Gradient Descent**: Optimizes the model by adjusting weights to minimize the error function.

## Classification vs Regression
- **Classification**: Predicts a discrete label (e.g., a cat or dog).
- **Regression**: Predicts a continuous outcome (e.g., house price).

## Practical Example
- Train on a dataset of house features to predict if a house is 'expensive' based on labeled data.
- Automatically categorize into 0 (not expensive) or 1 (expensive) through training and gradient descent.

## Logistic Regression in Machine Learning
- **Neurons in Neural Networks**: Act as building blocks, as logistic regression is used to create neurons for more complex models like neural networks.
- **Composable Functions**: Demonstrates the compositional nature of machine learning algorithms where functions are built on other functions (e.g., logistic built on linear).

## Resources
- **Andrew Ng's Coursera Course**: Continuing recommendation for in-depth learning on logistics and other machine learning models.